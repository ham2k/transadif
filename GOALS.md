# Prompt

We want to create a command-line tool in Rust that can read ADIF files, properly interpret the data into Unicode strings regardless of the original encoding, and then generate a valid ADIF output file according to the goals and considerations described in the rest of this GOALS.md file.

The project should include a mechanism to run the test cases provided and verify that the output is as expected.

The final result should be a redistributable standalone binary for Linux, Windows, and macOS.


# Goals

To provide a command-line tool written in Rust that can read ADIF files, properly interpret the data into Unicode strings regardless of the original encoding, and then generate a valid ADIF file in one of four possible encodings:

* UTF-8
* 8-bit characters using code pages such as ISO-8859-1, Windows-1252, etc.
* 7-bit ASCII

When encoding to 8-bit code pages, the tool should transcode compatible characters, and offer to replace incompatible characters with a single character ('?' by default), their entity reference ('&0xNN;'), or remove them.

When encoding to 7-bit ASCII, the tool should offer the choice to transliterate to characters without diacritics, replace incompatible characters with a single character ('?' by default), their entity reference ('&0xNN;'), or remove them.

The tool should be able to parse ADIF files encoded as 7-bit ASCII or 8-bit characters with a field count specified in bytes. And encoded as UTF-8 with a field count specified in characters, or in bytes, according to the heuristics described in "Field Counts" under "Considerations".

The tool should be able to detect entity references in the input data and replace them with the corresponding Unicode characters.

The tool should inspect any `data` containing byte values above 127 and try to correct any misencoded characters, according to the heuristics described in "Data Corrections" under "Considerations".


# Specification

ADIF files are text format files with a header and a sequence of records, each comprising a sequence of fields.

The header begins with any character other than '<' and ends with a case-insensitive '<eoh>'.

If the first character in an ADI file is '<', it contains no header. A header may contain arbitrary text of type String preserved as the header's `preamble`, as well as zero or more `header fields`.

Both header and record fields are of the form '<fieldname:length>data' or <fieldname:length:type>data'. `fieldname` and `type` are strings comprising 7-bit alphanumeric characters and underscores. `length` is a number expressed in ASCII digits. `data` is a string of `length` characters.

A field can be followed by arbitrary text until the next field and should be preserved as `excessdata`.

The next field is identified by having the correct sequence of characters, '<', `fieldname`, ':', `length`, optionally ':' and `type` and then '>'.

A record is a sequence of fields ending with a case-insensitive '<eor>'.

Any text between a '<eor>' tag and the next record should be preserved as the `excessdata` of the record.

Any text between the '<eoh>' tag and the first record should be preserved as the `excessdata` of the header.

ADIF files may specify an encoding using a header with a `fieldname` of"encoding".

Files generated by the tool must include this header.


# Considerations

Input files often do not specify a header, and even when they do, their strings might contain characters considered invalid in that encoding.

### Data Corrections

Input `data`strings should be inspected for invalid character sequences, and entity references, and corrected if possible, unless the tool was invoked in `--strict` mode. If running in `--strict` mode, the tool should report warnings for any invalid characters or entity references.

Regardless of a file's encoding, a field's `data` string should be inspected for entity references and these should be replaced by Unicode characters.

If a file is specified as encoded in ASCII or an 8-bit code page, and a field's `data` data contains sequences of two or more consecutive bytes above 127, and these bytes are valid UTF-8 sequences, then the `data` should be reinterpreted as UTF-8 instead of transcoded.

If a file is specified as encoded in UTF-8, but a field's `data` contains invalid UTF-8 characters that are considered valid ISO-8859-1 characters, and no other UTF-8 sequences, then this `data` string should be interpreted as ISO-8859-1 and transcoded to Unicode.

### Field counts

Field counts are normally specified in characters, according to the encoding of the input file. This means ASCII and 8-bit encoded files should specify the field count in bytes, and UTF-8 encoded files should specify the field count in Unicode code points (or characters, colloquially).

It is possible that a field count is specified in the wrong units for the given, or guessed, encoding.

If the tool finds that `excessdata` for a field is anything other than whitespace characters, and `data` includes UTF-8 sequences, it is possible that the field count should be interpreted as bytes instead of characters, or viceversa. If running in `--strict` mode, it should report a warning and continue with the next field. If not running in `--strict` mode, it should reinterpret the field count, and if that produces a result with `excessdata` containing only whitespace, it should use updated information this instead, and continue parsing the rest of the file with the new type of count. If the reinterpretation still contains non-whitespace characters, it should report a warning and continue with the next field.

### Implementation Details

The tool should be written in Rust.

The tool should be written in a way that is easy to test, and should be able to run the test cases provided.

The tool should be written in a way that is easy to distribute, and should be able to run on Linux, Windows, and macOS.

The tool should read input files as raw bytes, before interpreting the encoding of each string `data` value and converting it to Unicode.

The output ADIF file should calculate field counts based on the processed Unicode string values and the output encoding, and not the original field counts.

It should use the clap crate for command line parsing. It should use the htmlescape crate to manage HTML entity references. It should use the chardetng crate to detect the encoding of strings.

Include any relevant entries in gitignore.

To detect mojibake on a UTF-8 string, the tool should look for sequences of Unicode characters with code points between 192 and 223, followed by characters with code points between 128 and 191, and then verify if that sequence, when interpreted as bytes, corresponds to a valid UTF-8 sequences. If it does, the tool should replace that sequence by reinterpreting the bytes as UTF-8. This should be done recursively until no more sequences are found.


# Test cases

The test cases are in the `test-cases` directory, which can include subdirectories.

They are named like `001-in-plain.adi`, `001-out-plain.adi`, `002-in-plain-iso.adi`, `002-out-plain-iso.adi`, `003-in-plain-utf.adi`, `003-out-plain-utf.adi`.

The `in-` files are the input files, and the `out-` files are the expected output files.

Each input file has in the `preamble` section a command line invocation that should produce the expected output file. But it can ignore differences in the header preamble.

These test files might include invalid UTF-8 sequences, so the testing tool should open them as raw bytes or plain ASCII text when trying to find the command line definition and other metadata.

The comparison with the expected output file should be byte-by-byte, without using any encodings.

The test running tool should accept a parameter to run only specific tests whose filenames or directories match a given string.

And never create special logic in the code designed to hack around a specific test case.

# Usage

```
$ transadif <input-file> [options]
```

If no input file is specified, the tool should read from stdin.

## Options

* `-h, --help`: Show help information
* `-v, --version`: Show version information
* `-o, --output <output-file>`: Write the output to a file
* `-i, --input-encoding <encoding>`: Suggested encoding to use for the input file
* `-e, --encoding <encoding>`: Specify the encoding of the output file
* `-t, --transcode`: Transcode compatible characters
* `-r, --replace <character>`: Replace incompatible characters with a single character ('?' by default)
* `-d, --delete`: Delete incompatible characters
* `-a, --ascii`: Transliterate to characters without diacritics
* `-s, --strict`: Strict mode, do not correct invalid characters or field counts

## Strict mode

In strict mode, the tool will not correct invalid characters, and will instead report them as errors.

## Non-strict mode

In non-strict mode, the tool will correct invalid characters, and will report them as warnings.
